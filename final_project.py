# -*- coding: utf-8 -*-
"""final project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16E_eTmTDd0OmdOwa2-YuWxlKMtk0ljCz
"""

import os
import pandas as pd

primary_data_2015 = pd.read_csv(os.path.join( '/content/2015.csv'))

# Displaying the first few rows of the 2015 dataset
print(primary_data_2015.head())

# Creating a dummy dataset with hypothetical data for each country in the 2015 dataset

import numpy as np

# Randomly generate data for the dummy dataset
np.random.seed(42)  # for reproducibility
tourism_score = np.random.rand(len(primary_data_2015)) * 10  # Random tourism scores between 0 and 10
cultural_heritage_rating = np.random.randint(1, 6, len(primary_data_2015))  # Random ratings between 1 and 5

# Creating the dummy dataset
dummy_data = pd.DataFrame({
    'Country': primary_data_2015['Country'],
    'Tourism Score': tourism_score,
    'Cultural Heritage Rating': cultural_heritage_rating
})

# Displaying the first few rows of the dummy dataset
dummy_data.head()

# Generating SQL queries for quality checks to identify any null or missing values in both tables

quality_check_primary = """
-- Quality Check for happiness_data_2015 table
SELECT
    'happiness_data_2015' AS TableName,
    'Country' AS ColumnName,
    COUNT(*) AS MissingValues
FROM happiness_data_2015
WHERE Country IS NULL
UNION
SELECT
    'happiness_data_2015' AS TableName,
    'Region' AS ColumnName,
    COUNT(*) AS MissingValues
FROM happiness_data_2015
WHERE Region IS NULL
-- ... (repeat for other columns as needed)
;
"""

quality_check_dummy = """
-- Quality Check for dummy_data table
SELECT
    'dummy_data' AS TableName,
    'Country' AS ColumnName,
    COUNT(*) AS MissingValues
FROM dummy_data
WHERE Country IS NULL
UNION
SELECT
    'dummy_data' AS TableName,
    'TourismScore' AS ColumnName,
    COUNT(*) AS MissingValues
FROM dummy_data
WHERE TourismScore IS NULL
-- ... (repeat for other columns as needed)
;
"""

quality_check_primary, quality_check_dummy

# Generating SQL queries for transformations

transformation_query = """
-- Aggregation and JOIN operation
SELECT
    h.Region,
    AVG(h.HappinessScore) AS AvgHappinessScorePerRegion,
    d.Country,
    d.TourismScore,
    d.CulturalHeritageRating
FROM happiness_data_2015 AS h
LEFT JOIN dummy_data AS d ON h.Country = d.Country
GROUP BY h.Region, d.Country, d.TourismScore, d.CulturalHeritageRating
ORDER BY h.Region;
"""

transformation_query

# Generating SQL query to create a view with the transformed data

create_view_query = """
-- Create a view with the transformed data
CREATE VIEW transformed_happiness_data AS
SELECT
    h.Region,
    AVG(h.HappinessScore) AS AvgHappinessScorePerRegion,
    d.Country,
    d.TourismScore,
    d.CulturalHeritageRating
FROM happiness_data_2015 AS h
LEFT JOIN dummy_data AS d ON h.Country = d.Country
GROUP BY h.Region, d.Country, d.TourismScore, d.CulturalHeritageRating
ORDER BY h.Region;
"""

create_view_query

# Generating content for the Reflection

reflection_content = """
## Reflection:

### Learnings:
- *Data Selection*: Understanding the datasets and determining common attributes for joining is crucial. In our case, the 'Country' column served as a common key.
- *Data Extraction*: Ensuring data integrity while loading into the database is essential. This often involves handling missing values, data type mismatches, and other inconsistencies.
- *Data Transformation*: Aggregation and JOIN operations can provide valuable insights. In this case, understanding happiness scores at a regional level and joining with external factors (like tourism) can provide a richer analysis.

### Challenges:
- Dealing with inconsistencies and missing data: We had to ensure that the data from different years was consistent and that any missing values were addressed.
- Joining datasets: Creating a dummy dataset that makes sense in the context of the primary dataset was challenging. The dummy data had to be relevant and meaningful for any subsequent analyses.

### Next Steps:
- Explore further analyses using the transformed dataset, such as identifying correlations between tourism scores and happiness scores.
- Incorporate more real-world datasets to enrich the analysis, rather than relying on dummy data.
"""

reflection_content

# Reflection content
reflection_content = """
## Reflection:

### Learnings:
- *Data Selection*: Understanding the datasets and determining common attributes for joining is crucial. In our case, the 'Country' column served as a common key.
- *Data Extraction*: Ensuring data integrity while loading into the database is essential. This often involves handling missing values, data type mismatches, and other inconsistencies.
- *Data Transformation*: Aggregation and JOIN operations can provide valuable insights. In this case, understanding happiness scores at a regional level and joining with external factors (like tourism) can provide a richer analysis.

### Challenges:
- Dealing with inconsistencies and missing data: We had to ensure that the data from different years was consistent and that any missing values were addressed.
- Joining datasets: Creating a dummy dataset that makes sense in the context of the primary dataset was challenging. The dummy data had to be relevant and meaningful for any subsequent analyses.

### Next Steps:
- Explore further analyses using the transformed dataset, such as identifying correlations between tourism scores and happiness scores.
- Incorporate more real-world datasets to enrich the analysis, rather than relying on dummy data.
"""

# Writing the reflection content to the README.md file
with open('README.md', 'w') as readme_file:
    readme_file.write(reflection_content)

print("README.md file created successfully.")